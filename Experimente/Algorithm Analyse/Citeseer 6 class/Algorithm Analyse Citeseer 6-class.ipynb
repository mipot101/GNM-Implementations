{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import copy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # Lokales Speichern von Objekten\n",
    "import keyboard\n",
    "from scipy import interpolate\n",
    "\n",
    "from GNM_Toolbox import *\n",
    "\n",
    "dataset = load_dataset('Citeseer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gegeben sei eine target_list (a_0, a_1, a_2, ...)\n",
    "# und eine out_list ((b_0, x), (b_1, x), (b_2, x), (b_3, x), ...)\n",
    "# out_list darf tupel beliebiger Länge haben. \n",
    "# Gesucht wird eine Liste l von Indizes, sodass für i < len(target_list): abs(target_list[i] - out_list[l[i]][0]) minimal ist\n",
    "def find_each_nearest(target_list, out_list, idx = 0):\n",
    "    # Each list is expected to be sorted\n",
    "    i, j = 0, 0\n",
    "    result = list()\n",
    "    while True:\n",
    "        diff_0 = abs(target_list[i] - out_list[j][idx])\n",
    "        diff_1 = abs(target_list[i] - out_list[j+1][idx])\n",
    "        \n",
    "        if diff_0 >= diff_1:\n",
    "            j += 1\n",
    "        elif diff_0 < diff_1:\n",
    "            result.append(j)\n",
    "            i += 1\n",
    "        if i >= len(target_list):\n",
    "            return result\n",
    "        if j+1 >= len(out_list):\n",
    "            while i < len(target_list):\n",
    "                result.append(j)\n",
    "                i += 1\n",
    "            return result\n",
    "            \n",
    "def get_best_values_indices(targets, lambdas):\n",
    "    lambdas.sort(key = lambda x: x[0])\n",
    "    return find_each_nearest(targets, lambdas)\n",
    "\n",
    "def create_mask_from_pi(data, pi):\n",
    "    p = pi(data.x, data.y)\n",
    "    mask = torch.tensor((np.random.binomial(size = p.shape[0], n = 1, p = p) == 1))        \n",
    "    return mask.bool()\n",
    "\n",
    "def split_known_mask_into_val_and_train_mask(known, ratio=0.8):\n",
    "    val_mask = torch.zeros_like(known) == 1\n",
    "    train_mask = torch.zeros_like(known) == 1\n",
    "    for i in range(len(known)):\n",
    "        if known[i] == True:\n",
    "            if np.random.binomial(1, ratio) == 1:\n",
    "                train_mask[i] = True\n",
    "            else:\n",
    "                val_mask[i] = True\n",
    "    return val_mask, train_mask\n",
    "\n",
    "def count_classes(y, mask):\n",
    "    l = np.zeros((max(y)+1))\n",
    "    for yy in y[mask]:\n",
    "        l[yy] += 1\n",
    "    return l\n",
    "\n",
    "def calc_variance(y, mask):\n",
    "    y_distribution = count_classes(y, mask)\n",
    "    return np.var(y_distribution)\n",
    "        \n",
    "def insert_into_list(l, item, t):\n",
    "    # l list, i item to insert, target\n",
    "    def diff(a, b):\n",
    "        return abs(a-b)\n",
    "    N = len(l)\n",
    "    if N == 0:\n",
    "        l.insert(0, item)\n",
    "        return\n",
    "    d = diff(t, item[0])\n",
    "    d_0 = diff(t, l[0][0])\n",
    "    if d <= d_0:\n",
    "        l.insert(0, item)\n",
    "        return\n",
    "    for i in range(N-1):\n",
    "        d_0 = diff(t, l[i][0])\n",
    "        d_1 = diff(t, l[i+1][0])\n",
    "        if d_0 <= d and d <= d_1:\n",
    "            l.insert(i+1, item)\n",
    "            return\n",
    "    l.append(item)\n",
    "\n",
    "def disturb_y(y, probability): # y must have 2 classes\n",
    "    y_dis = torch.zeros_like(y)\n",
    "    for i in range(len(y)):\n",
    "        if np.random.rand(1) < probability:\n",
    "            y_dis[i] = 0 if np.random.rand(1) > 0.5 else 1\n",
    "        else:\n",
    "            y_dis[i] = y[i]\n",
    "                \n",
    "    return F.one_hot(y_dis, 2)\n",
    "\n",
    "# root mean squared error\n",
    "def RMSE(pi_est, pi_true):\n",
    "    return torch.sqrt(F.mse_loss(pi_true, pi_est.view(pi_true.shape)))\n",
    "\n",
    "# mean squared error\n",
    "def MSE(pi_est, pi_true):\n",
    "    return F.mse_loss(pi_true, pi_est)\n",
    "\n",
    "# weighted mean squared error\n",
    "def WMSE(pi_est, pi_true, weight):\n",
    "    return torch.mean(weight * (pi_est - pi_true) ** 2)\n",
    "\n",
    "\n",
    "# mean root error\n",
    "def MRE(pi_est, pi_true):\n",
    "    return torch.mean(torch.pow(abs(pi_est.view(pi_true.shape) - pi_true), 0.5))\n",
    "\n",
    "# berechnet Gewichtungen für weighted mean square error\n",
    "def calc_inverse_weights(pi_real):\n",
    "    bins = torch.zeros(len(pi_real)).type(torch.float)\n",
    "    pi = pi_real.detach().numpy()\n",
    "    # Count \n",
    "    for p in pi_real:\n",
    "        bins[int(p*100)] += 1\n",
    "    bins = 1/bins\n",
    "    bins[bins == (torch.tensor(1.)/0)] = 999999999\n",
    "    return torch.sqrt(bins)\n",
    "\n",
    "def eval_pi(pi_true, pi_est, diff=0.01):\n",
    "    pi_diff = abs(pi_true.view(pi_est.shape)-pi_est)\n",
    "    pi_binar = [(p < diff) for p in pi_diff]\n",
    "    return sum(pi_binar)/(1. * pi_true.shape[0])\n",
    "\n",
    "def var(l, mean):\n",
    "    # l: list of floats, \n",
    "    # mean: float\n",
    "    return np.mean((l-mean)**2)\n",
    "\n",
    "def plot_data(all_models, colors):\n",
    "    names = list(all_models.keys())\n",
    "    M = len(names)\n",
    "    \n",
    "    plt.figure(None, (18, 8))\n",
    "    \n",
    "    for name, color in zip(names, colors):\n",
    "        # print point cloud\n",
    "        data = np.array(all_models[name]).T\n",
    "        plt.plot(*data, 'x', alpha = 1, color=color)\n",
    "        \n",
    "        # print mean function\n",
    "        lambdas = data[0]\n",
    "        values = data[1]\n",
    "        mini = np.min(lambdas)\n",
    "        maxi = np.max(lambdas)\n",
    "        width = (maxi-mini)/5\n",
    "        xn = np.linspace(mini, maxi, 30)\n",
    "        yn_mean = np.zeros_like(xn)\n",
    "        yn_lower_var = np.zeros_like(xn)\n",
    "        yn_upper_var = np.zeros_like(xn)\n",
    "        for i in range(len(xn)):\n",
    "            ys = list()\n",
    "            # Find all values in given interval [xn[i] +- width/2]\n",
    "            for j in range(len(values)):\n",
    "                if lambdas[j] <= xn[i] + width/2 and lambdas[j] >= xn[i] - width/2:\n",
    "                    ys.append(values[j])\n",
    "            ys = np.sort(ys)\n",
    "            mean = np.mean(ys)\n",
    "            ys_lower = [y for y in ys if y < mean]\n",
    "            ys_upper = [y for y in ys if y > mean]\n",
    "            \n",
    "            yn_mean[i] = mean\n",
    "            yn_lower_var[i] = np.sqrt(var(ys_lower, mean))\n",
    "            yn_upper_var[i] = np.sqrt(var(ys_upper, mean))\n",
    "            \n",
    "            \n",
    "        mean_spline = interpolate.UnivariateSpline(xn, yn_mean)\n",
    "        var_upper_spline = interpolate.UnivariateSpline(xn, yn_mean + yn_upper_var)\n",
    "        var_lower_spline = interpolate.UnivariateSpline(xn, yn_mean - yn_lower_var)\n",
    "        mean_spline.set_smoothing_factor(0.0001)\n",
    "        var_upper_spline.set_smoothing_factor(0.001)\n",
    "        var_lower_spline.set_smoothing_factor(0.001)\n",
    "        \n",
    "        #plt.plot(xn, yn_mean, color=color, label=name)\n",
    "        plt.plot(xn, mean_spline(xn), color=color, label=name)\n",
    "        \n",
    "        plt.plot(xn, var_lower_spline(xn), alpha=0.3, color=color)\n",
    "        plt.plot(xn, var_upper_spline(xn), alpha=0.3, color=color)\n",
    "        plt.fill_between(xn, var_lower_spline(xn), var_upper_spline(xn), alpha=0.04, color=color)\n",
    "\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.xlabel(r'$\\lambda$', fontsize=20)\n",
    "    plt.ylabel(r'Genauigkeit', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    a0, a1, a2, a3 = 13, 4, 15, 15\n",
    "    return torch.exp(torch.sum(x, dim=1)/a0 - a1) - ((torch.sum(x, dim=1) - a2 ) / a3)\n",
    "    \n",
    "def pi(X, y):\n",
    "    y0 = F.one_hot(y, 6)\n",
    "    # y should be one-hot encoded\n",
    "    a0, a1, a2 = -torch.log(torch.tensor(20.)), 1, torch.tensor([[5, 0.5, 0.2, 1, 0.5, 2]]).view((6, 1))\n",
    "    return torch.sigmoid(a0 + a1 * h(X).view((-1, 1)) + y0.type(torch.float) @ a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = dataset[0].to(device)\n",
    "data.num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finde Masken mit passenden Lambdas\n",
    "# Erstelle dazu 2000 Masken und wähle dann die Masken aus, dessen Lambdas den targets entsprechen\n",
    "masks0 = list()\n",
    "for i in range(200):\n",
    "    mask = create_mask_from_pi(data, pi)\n",
    "    val_mask, train_mask = split_known_mask_into_val_and_train_mask(mask)\n",
    "    l = calc_variance(data.y, train_mask)\n",
    "    masks0.append([train_mask, val_mask, l])\n",
    "    print(i, end='\\r')\n",
    "masks0.sort(key=lambda x: x[2])\n",
    "targets = np.linspace(10, 120, 200) # choose masks depending on lambdas\n",
    "idxs = find_each_nearest(targets, masks0, idx=2)\n",
    "idxs = list(dict.fromkeys(idxs)) # remove duplicates\n",
    "masks = [masks0[i] for i in idxs]\n",
    "print('Got {} masks'.format(len(masks)))\n",
    "#np.random.shuffle(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_masks = masks\n",
    "IT_PER_MASK = 2\n",
    "M = len(choosen_masks)\n",
    "sm_models = list()\n",
    "gnm_models = list()\n",
    "gnma_models = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteriere über Masken\n",
    "t_0 = time.time()\n",
    "for j, mask_tupel in enumerate(choosen_masks):\n",
    "    train_mask, val_mask, l = mask_tupel\n",
    "\n",
    "    # Trainiere jeweils N Modelle\n",
    "    for k in range(IT_PER_MASK):\n",
    "        print_status(j * IT_PER_MASK + k, M * IT_PER_MASK, t_0)\n",
    "    \n",
    "        # In gnm.py wird beim Sampling die Methode np.random.multinomial.\n",
    "        # Aufgrund von der Ungenauigkeit von Floats/Doubles kommt es hier in sehr seltenen Fälle vor, dass\n",
    "        # die Summe über die Wahrscheinlichkeiten 1 überschreitet und somit ein Fehler geworfen wird.\n",
    "        # Damit das den durchaus langen Prozess des Trainings nicht abbricht, wird dieser hier ggf abgefangen.\n",
    "        try:\n",
    "            _, _, acc_sm, _ = train_one_net(data, train_mask, val_mask)\n",
    "            sm_models.append([l, acc_sm])\n",
    "        except Exception as e: \n",
    "            print('SM: {}'.format(e))\n",
    "        \n",
    "        try:\n",
    "            _, _, acc_gnm = train_net_with_gnm(data, train_mask, val_mask)\n",
    "            gnm_models.append([l, acc_gnm[-1]])\n",
    "        except Exception as e: \n",
    "            print('GNM: {}'.format(e))\n",
    "            \n",
    "        try:\n",
    "            _, _, acc_gnma = train_net_with_gnm_adapted(data, train_mask, val_mask)\n",
    "            gnma_models.append([l, acc_gnma])\n",
    "        except Exception as e: \n",
    "            print('GNMn: {}'.format(e))\n",
    "        \n",
    "        \n",
    "        \n",
    "        all_models = {'GNM': gnm_models, 'GNMn': gnma_models, 'SM': sm_models}\n",
    "#pickle_write('algorithm-analysis-citeseer-2.pkl', all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_models)\n",
    "plot_data(all_models, ['red', 'blue', 'green']) # Hierfür müssen ausreichend Daten vorhanden sein."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
