{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset on harddrive.\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import copy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # Lokales Speichern von Objekten\n",
    "import keyboard\n",
    "from collections import Counter\n",
    "\n",
    "from GNM_Toolbox.tools.tools import *\n",
    "from GNM_Toolbox.gnm import *\n",
    "from GNM_Toolbox.data.dataloader import *\n",
    "\n",
    "dataset = load_dataset('Citeseer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gegeben sei eine target_list (a_0, a_1, a_2, ...)\n",
    "# und eine out_list ((b_0, x), (b_1, x), (b_2, x), (b_3, x), ...)\n",
    "# Gesucht wird eine Liste l von Indizes, \n",
    "# sodass für i < len(target_list): abs(target_list[i] - out_list[l[i]][0]) minimal ist\n",
    "def find_each_nearest(target_list, out_list):\n",
    "    # Each list is expected to be sorted\n",
    "    i, j = 0, 0\n",
    "    result = list()\n",
    "    while True:\n",
    "        diff_0 = abs(target_list[i] - out_list[j][0])\n",
    "        diff_1 = abs(target_list[i] - out_list[j+1][0])\n",
    "        \n",
    "        if diff_0 >= diff_1:\n",
    "            j += 1\n",
    "        elif diff_0 < diff_1:\n",
    "            result.append(j)\n",
    "            i += 1\n",
    "        if i >= len(target_list):\n",
    "            return result\n",
    "        if j+1 >= len(out_list):\n",
    "            while i < len(target_list):\n",
    "                result.append(j)\n",
    "                i += 1\n",
    "            return result\n",
    "            \n",
    "def get_best_values_indices(targets, lambdas):\n",
    "    lambdas.sort(key = lambda x: x[0])\n",
    "    return find_each_nearest(targets, lambdas)\n",
    "\n",
    "def h(x):\n",
    "    a0, a1, a2, a3 = 13, 4, 15, 15\n",
    "    return torch.exp(torch.sum(x, dim=1)/a0 - a1) - ((torch.sum(x, dim=1) - a2 ) / a3)\n",
    "    \n",
    "def pi(X, y):\n",
    "    y0 = F.one_hot(y, 2)\n",
    "    # y should be one-hot encoded\n",
    "    a0, a1, a2 = -torch.log(torch.tensor(20.)), 1, torch.tensor([[1, 0.5]]).view((2, 1))\n",
    "    return torch.sigmoid(a0 + a1 * h(X).view((-1, 1)) + y0.type(torch.float) @ a2)\n",
    "\n",
    "def create_mask_from_pi(data, pi):\n",
    "    p = pi(data.x, data.y)\n",
    "    mask = torch.tensor((np.random.binomial(size = p.shape[0], n = 1, p = p) == 1))        \n",
    "    return mask.bool()\n",
    "\n",
    "def split_known_mask_into_val_and_train_mask(known, ratio=0.8):\n",
    "    val_mask = torch.zeros_like(known) == 1\n",
    "    train_mask = torch.zeros_like(known) == 1\n",
    "    for i in range(len(known)):\n",
    "        if known[i] == True:\n",
    "            if np.random.binomial(1, ratio) == 1:\n",
    "                train_mask[i] = True\n",
    "            else:\n",
    "                val_mask[i] = True\n",
    "    return val_mask, train_mask\n",
    "\n",
    "def count_classes(y, mask):\n",
    "    l = np.zeros((max(y)+1))\n",
    "    for yy in y[mask]:\n",
    "        l[yy] += 1\n",
    "    return l\n",
    "\n",
    "def calc_variance(y, mask):\n",
    "    y_distribution = count_classes(y, mask)\n",
    "    return np.var(y_distribution)\n",
    "        \n",
    "def insert_into_list(l, item, t):\n",
    "    # l list, i item to insert, target\n",
    "    def diff(a, b):\n",
    "        return abs(a-b)\n",
    "    N = len(l)\n",
    "    if N == 0:\n",
    "        l.insert(0, item)\n",
    "        return\n",
    "    d = diff(t, item[0])\n",
    "    d_0 = diff(t, l[0][0])\n",
    "    if d <= d_0:\n",
    "        l.insert(0, item)\n",
    "        return\n",
    "    for i in range(N-1):\n",
    "        d_0 = diff(t, l[i][0])\n",
    "        d_1 = diff(t, l[i+1][0])\n",
    "        if d_0 <= d and d <= d_1:\n",
    "            l.insert(i+1, item)\n",
    "            return\n",
    "    l.append(item)\n",
    "    \n",
    "def alternative_boxplot(pi_est, data, x_position=1, x_width=1):\n",
    "    # data: list of floats\n",
    "    # x_postition: float\n",
    "    # x_width: float\n",
    "    N = len(pi_est)\n",
    "    mean = np.mean(pi_est)\n",
    "    print('{}:{}'.format(sum([1 for d in pi_est if d > mean]), sum([1 for d in pi_est if d < mean])))\n",
    "    plt.plot([x_position-x_width/2, x_position-x_width/2], [mean, mean], '-', color='grey')\n",
    "    noise = np.random.rand(N)\n",
    "    x_positions = x_position - x_width/2. + noise * x_width\n",
    "    plt.plot(x_positions[data.y == 0], np.array(pi_est[data.y == 0]), '.', color = 'red', label='0')\n",
    "    plt.plot(x_positions[data.y == 1], np.array(pi_est[data.y == 1]), '.', color = 'blue', label='1')\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "def disturb_y(y, probability): # y must have 2 classes\n",
    "    y_dis = torch.zeros_like(y)\n",
    "    for i in range(len(y)):\n",
    "        if np.random.rand(1) < probability:\n",
    "            y_dis[i] = np.random.randint(2)\n",
    "        else:\n",
    "            y_dis[i] = y[i]\n",
    "                \n",
    "    return F.one_hot(y_dis, 2)\n",
    "\n",
    "\n",
    "def eval_pi(pi_true, pi_est, diff=0.01):\n",
    "    pi_diff = abs(pi_true.view(pi_est.shape)-pi_est)\n",
    "    pi_binar = [(p < diff) for p in pi_diff]\n",
    "    return sum(pi_binar)/(1. * pi_true.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Informations:\n",
    "\n",
    "Citeseer Datasets consists of 6 classes.\n",
    "\n",
    "Node distribution is as follows 249, 590, 668, 701, 596, 523.\n",
    "\n",
    "Therefore we set new class 0 to be class 3. New class 1 is going to be the rest.\n",
    "\n",
    "We have 3327 nodes with 3703 features each. \n",
    "\n",
    "Citation:\n",
    "@inproceedings{nr,\n",
    "     title={The Network Data Repository with Interactive Graph Analytics and Visualization},\n",
    "     author={Ryan A. Rossi and Nesreen K. Ahmed},\n",
    "     booktitle={AAAI},\n",
    "     url={http://networkrepository.com},\n",
    "     year={2015}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = dataset[0].to(device)\n",
    "data.num_classes = 2\n",
    "threes = data.y == 3\n",
    "data.y = torch.ones_like(data.y)\n",
    "data.y[threes] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verteilung von $pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 50)\n",
    "pi_true = pi(data.x, data.y)\n",
    "pi_true = pi_true.view((pi_true.shape[0],))\n",
    "print(pi_true.shape)\n",
    "plt.hist(pi_true[data.y == 1], bins, alpha=0.6, color='red', edgecolor='black', label='1');\n",
    "plt.hist(pi_true[data.y == 0], bins, alpha=0.6, color='blue', edgecolor='black', label='0');\n",
    "plt.legend();\n",
    "mask = create_mask_from_pi(data, pi)\n",
    "print('M1 = {:.3f} \\nM2 = {:.3f} \\nPerc = {:.3f}'.format(torch.mean(pi_true[mask]), \n",
    "                                            torch.mean(pi_true[mask.logical_not()]),\n",
    "                                            sum(mask)/(1. * len(data.y))))\n",
    "plt.figure()\n",
    "plt.hist(pi_true[(data.y == 1) & mask], bins, alpha=0.6, color='red', edgecolor='black', label='1');\n",
    "plt.hist(pi_true[(data.y == 0) & mask], bins, alpha=0.6, color='blue', edgecolor='black', label='0');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 40 Train and Val Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "masks1 = list()\n",
    "for i in range(80):\n",
    "    mask1 = create_mask_from_pi(data, pi)\n",
    "    val_mask1, train_mask1 = split_known_mask_into_val_and_train_mask(mask1)\n",
    "    masks1.append((None, train_mask1, val_mask1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_masks = {1: masks1}\n",
    "pi_true = pi(data.x, data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160/160) |████████████████████| (7h 50min 10sec|2min 57sec|2min 57sec)                                    \r"
     ]
    }
   ],
   "source": [
    "# Eval \n",
    "IT_per_mask = 2\n",
    "NB_masks = len(choosen_masks[1])\n",
    "M = len(choosen_masks)\n",
    "t_0 = time.time()\n",
    "noise_levels = [0, 0.1, 0.2, 0.4, 0.8, 1.1]\n",
    "all_models = dict()\n",
    "test = None\n",
    "\n",
    "# Iteriere über Masken\n",
    "for i, l in enumerate(choosen_masks):\n",
    "    sms_models = list() # SM Standard\n",
    "    smn_models = [list() for _ in range(len(noise_levels))] # change to len(noise_levels)\n",
    "    \n",
    "    for j, mask_tupel in enumerate(choosen_masks[l]):\n",
    "        _, train_mask, val_mask = mask_tupel\n",
    "\n",
    "        # Trainiere jeweils N Modelle\n",
    "        for k in range(IT_per_mask):\n",
    "            print_status(i * NB_masks * IT_per_mask + j * IT_per_mask + k, M * NB_masks * IT_per_mask, t_0)\n",
    "            for m, noise_level in enumerate(noise_levels):\n",
    "                y_noise = disturb_y(data.y, noise_level)\n",
    "                pi_est, loss_list, acc_r, _ = train_modelr4(data.x, data.y, data.num_classes, train_mask, val_mask, y_noise, expectation='other')\n",
    "                \n",
    "                smn_models[m].append(loss_list[-1])\n",
    "                    \n",
    "        all_models[l] = (smn_models)\n",
    "        #pickle_write('l2_analysis-noise-1/run-{}.pkl'.format(i), all_models)\n",
    "    all_models[l] = (smn_models)\n",
    "    #pickle_write('l2_analysis-noise-1/run-{}.pkl'.format(i), all_models)\n",
    "pickle_write('l2-improve-citeseer-2.pkl', all_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
