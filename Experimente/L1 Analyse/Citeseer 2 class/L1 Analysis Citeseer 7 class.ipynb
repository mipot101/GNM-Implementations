{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset on harddrive.\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import copy\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # Lokales Speichern von Objekten\n",
    "import keyboard\n",
    "from collections import Counter\n",
    "\n",
    "from GNM_Toolbox.tools.tools import *\n",
    "from GNM_Toolbox.gnm import *\n",
    "from GNM_Toolbox.data.dataloader import *\n",
    "\n",
    "dataset = load_dataset('Citeseer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gegeben sei eine target_list (a_0, a_1, a_2, ...)\n",
    "# und eine out_list ((b_0, x), (b_1, x), (b_2, x), (b_3, x), ...)\n",
    "# Gesucht wird eine Liste l von Indizes, sodass f√ºr i < len(target_list): abs(target_list[i] - out_list[l[i]][0]) minimal ist\n",
    "def find_each_nearest(target_list, out_list):\n",
    "    # Each list is expected to be sorted\n",
    "    i, j = 0, 0\n",
    "    result = list()\n",
    "    while True:\n",
    "        diff_0 = abs(target_list[i] - out_list[j][0])\n",
    "        diff_1 = abs(target_list[i] - out_list[j+1][0])\n",
    "        \n",
    "        if diff_0 >= diff_1:\n",
    "            j += 1\n",
    "        elif diff_0 < diff_1:\n",
    "            result.append(j)\n",
    "            i += 1\n",
    "        if i >= len(target_list):\n",
    "            return result\n",
    "        if j+1 >= len(out_list):\n",
    "            while i < len(target_list):\n",
    "                result.append(j)\n",
    "                i += 1\n",
    "            return result\n",
    "            \n",
    "def get_best_values_indices(targets, lambdas):\n",
    "    lambdas.sort(key = lambda x: x[0])\n",
    "    return find_each_nearest(targets, lambdas)\n",
    "\n",
    "def h(x):\n",
    "    a0, a1, a2, a3 = 13, 4, 15, 15\n",
    "    return torch.exp(torch.sum(x, dim=1)/a0 - a1) - ((torch.sum(x, dim=1) - a2 ) / a3)\n",
    "    \n",
    "def pi(X, y):\n",
    "    a0, a1, a2 = -torch.log(torch.tensor(4.)), 1, -1\n",
    "    return torch.sigmoid(a0 + a1 * h(X) + a2 * y)\n",
    "\n",
    "def create_mask_from_pi(data, pi):\n",
    "    p = pi(data.x, data.y)\n",
    "    mask = torch.tensor((np.random.binomial(size = p.shape[0], n = 1, p = p) == 1))        \n",
    "    return mask.bool()\n",
    "\n",
    "def split_known_mask_into_val_and_train_mask(known, ratio=0.8):\n",
    "    val_mask = torch.zeros_like(known) == 1\n",
    "    train_mask = torch.zeros_like(known) == 1\n",
    "    for i in range(len(known)):\n",
    "        if known[i] == True:\n",
    "            if np.random.binomial(1, ratio) == 1:\n",
    "                train_mask[i] = True\n",
    "            else:\n",
    "                val_mask[i] = True\n",
    "    return val_mask, train_mask\n",
    "\n",
    "def calculate_lambda(train_mask, y):\n",
    "    a = 0 # Anzahl an Klasse 0\n",
    "    b = 0 # Anzahl an Klasse 1\n",
    "    for yy in y[train_mask]:\n",
    "        if yy == 0:\n",
    "            a += 1\n",
    "        elif yy == 1:\n",
    "            b += 1\n",
    "    return b/a\n",
    "        \n",
    "def insert_into_list(l, item, t):\n",
    "    # l list, i item to insert, target\n",
    "    def diff(a, b):\n",
    "        return abs(a-b)\n",
    "    N = len(l)\n",
    "    if N == 0:\n",
    "        l.insert(0, item)\n",
    "        return\n",
    "    d = diff(t, item[0])\n",
    "    d_0 = diff(t, l[0][0])\n",
    "    if d <= d_0:\n",
    "        l.insert(0, item)\n",
    "        return\n",
    "    for i in range(N-1):\n",
    "        d_0 = diff(t, l[i][0])\n",
    "        d_1 = diff(t, l[i+1][0])\n",
    "        if d_0 <= d and d <= d_1:\n",
    "            l.insert(i+1, item)\n",
    "            return\n",
    "    l.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Informations:\n",
    "\n",
    "Citeseer Datasets consists of 6 classes.\n",
    "\n",
    "Node distribution is as follows 249, 590, 668, 701, 596, 523.\n",
    "\n",
    "Therefore we set new class 0 to be class 3. New class 1 is going to be the rest.\n",
    "\n",
    "We have 3327 nodes with 3703 features each. \n",
    "\n",
    "Citation:\n",
    "@inproceedings{nr,\n",
    "     title={The Network Data Repository with Interactive Graph Analytics and Visualization},\n",
    "     author={Ryan A. Rossi and Nesreen K. Ahmed},\n",
    "     booktitle={AAAI},\n",
    "     url={http://networkrepository.com},\n",
    "     year={2015}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = dataset[0].to(device)\n",
    "y_tmp = data.y == 3\n",
    "data.y = torch.ones_like(data.y)\n",
    "data.y[y_tmp] = 0\n",
    "data.num_classes = 2\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-97bc2e91eedc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlambdas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambdas = list()\n",
    "\n",
    "for i in range(1000):\n",
    "    mask = create_mask_from_pi(data, pi)\n",
    "    var_lambda = calculate_lambda(mask, data.y)\n",
    "    lambdas.append(var_lambda)\n",
    "    print('{:.3f}'.format(sum(mask)/(len(mask)*1.0)), end='\\r')\n",
    "    time.sleep(1)\n",
    "\n",
    "plt.hist(lambdas, bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Fitting Train and Val Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find masks such that lambda has specific values\n",
    "targets = [1, 1.5, 2]\n",
    "masks = dict()\n",
    "fitting_masks = dict()\n",
    "targets_done = set()\n",
    "worst_allowed_diff = 0.01\n",
    "worst_diff = 1\n",
    "num_masks = 40 # num_masks_per_lambda\n",
    "\n",
    "for t in targets:\n",
    "    masks[t] = list()\n",
    "    fitting_masks[t] = 0\n",
    "    \n",
    "pickle_write('masks_citeseer_small.pkl', (masks, fitting_masks, targets_done))\n",
    "\n",
    "def print_fitting(fitting_masks, i):\n",
    "    k = fitting_masks.keys()\n",
    "    string = ''\n",
    "    for kk in k:\n",
    "        if fitting_masks[kk] <= 40:\n",
    "            string += ' {}'.format(fitting_masks[kk])\n",
    "        else:\n",
    "            string += ' 40+'\n",
    "    print(string + ' at iteration {}'.format(i), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40 40+ 40 at iteration 24075\r"
     ]
    }
   ],
   "source": [
    "masks, fitting_masks, targets_done = pickle_read('masks_citeseer_small.pkl')\n",
    "i = 0\n",
    "t_start = time.time()\n",
    "while worst_diff > worst_allowed_diff and i < 40000 and len(set(targets)-targets_done) > 0:\n",
    "    i += 1\n",
    "    known_mask = create_mask_from_pi(data, pi)\n",
    "    val_mask, train_mask = split_known_mask_into_val_and_train_mask(known_mask)\n",
    "    l = calculate_lambda(train_mask, data.y)\n",
    "    item = (l, train_mask, val_mask)\n",
    "    \n",
    "    # Update masks\n",
    "    for t in set(targets)-targets_done:\n",
    "        diff = abs(l - t)\n",
    "        masks_ind = masks[t]\n",
    "        if len(masks_ind) < num_masks:\n",
    "            insert_into_list(masks_ind, item, t)\n",
    "            if diff < worst_allowed_diff:\n",
    "                fitting_masks[t] += 1\n",
    "                #print_fitting(fitting_masks, i)\n",
    "                if fitting_masks[t] > 40:\n",
    "                    targets_done.add(t)\n",
    "        elif abs(masks_ind[num_masks-1][0] - t) > diff:\n",
    "            insert_into_list(masks_ind, item, t)\n",
    "            masks[t] = masks_ind[0:num_masks]\n",
    "            if diff < worst_allowed_diff:\n",
    "                fitting_masks[t] += 1\n",
    "                print_fitting(fitting_masks, i)\n",
    "                if fitting_masks[t] > 40:\n",
    "                    targets_done.add(t)\n",
    "    \n",
    "    # Update worst_diff\n",
    "    worst_diff_old = worst_diff\n",
    "    worst_diff = 0\n",
    "    for t in set(targets)-targets_done:\n",
    "        length = len(masks[t])\n",
    "        diff = abs(t - masks[t][length-1][0])\n",
    "        if diff > worst_diff:\n",
    "            worst_diff = diff\n",
    "            \n",
    "# Safe masks\n",
    "pickle_write('masks_citeseer_small.pkl', (masks, fitting_masks, targets_done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis if Loss Function $L_1$ ist actually better than NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate actuall pi\n",
    "pi_true = pi(data.x, data.y)\n",
    "\n",
    "# Load masks\n",
    "all_masks,_,_ = pickle_read('masks_citeseer_small.pkl')\n",
    "subset = [1, 1.5, 2]\n",
    "choosen_masks = {k: all_masks[k] for k in subset}\n",
    "\n",
    "# Drei Masken: 10, 15, 20\n",
    "# Acht +1 Noiselevel: 0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480/480) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (2h 15min 50sec|0.00sec|17.01sec)                                     \r"
     ]
    }
   ],
   "source": [
    "# Eval 'Perfect' Loss, 'Perfect' Loss with noise and NLL\n",
    "IT_per_mask = 4\n",
    "NB_masks = len(choosen_masks[1.0])\n",
    "M = len(choosen_masks)\n",
    "t_0 = time.time()\n",
    "noise_levels = [0, 0.0025, 0.005, 0.0075, 0.01, 0.0125, 0.015, 0.0175, 0.02]\n",
    "all_models = dict()\n",
    "\n",
    "# Iteriere √ºber Masken\n",
    "for i, l in enumerate(choosen_masks):\n",
    "    sms_models = list() # SM Standard\n",
    "    smn_models = [list() for _ in range(9)] # SM Advanced with noises\n",
    "    \n",
    "    for j, mask_tupel in enumerate(choosen_masks[l]):\n",
    "        _, train_mask, val_mask = mask_tupel\n",
    "\n",
    "        # Trainiere jeweils N Modelle\n",
    "        for k in range(IT_per_mask):\n",
    "            print_status(i * NB_masks * IT_per_mask + j * IT_per_mask + k, M * NB_masks * IT_per_mask, t_0)\n",
    "            noise = torch.randn_like(pi_true)\n",
    "            for m, noise_level in enumerate(noise_levels):\n",
    "                pi = pi_true + noise_level * noise\n",
    "                smn_models[m].append((*train_one_net(data, \n",
    "                                                  train_mask, \n",
    "                                                  val_mask,\n",
    "                                                  loss_function=weighted_categorial_crossentropy_loss(pi[train_mask], reduction='mean'),\n",
    "                                                  val_loss_function=weighted_categorial_crossentropy_loss(pi[val_mask], reduction='mean'))[1:], j))\n",
    "            sms_models.append((*train_one_net(data, train_mask, val_mask)[1:], j))\n",
    "        all_models[l] = (sms_models, *smn_models)\n",
    "        pickle_write('l1_analysis-citeseer_small.pkl', all_models)\n",
    "    all_models[l] = (sms_models, *smn_models)\n",
    "    \n",
    "pickle_write('l1_analysis-citeseer_small.pkl', all_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
